# .env.example
# Copy this file to .env and fill in the values.
# ----------------------------------------------------

# --- Backend Configuration ---
# Use a PostgreSQL connection string for production.
# For local development with docker-compose, this will connect to the 'db' service.
DATABASE_URL=postgresql://user:password@db:5432/voicegenie_db

# --- Self-Hosted AI Services ---
# The URL for your self-hosted Ollama instance
OLLAMA_HOST=http://ollama:11434

# The name of the LLM model to use within Ollama (e.g., "mistral", "llama2")
LLM_MODEL_NAME=mistral

# The size of the Whisper model to use ('tiny', 'base', 'small', 'medium', 'large')
# Smaller models are faster but less accurate. 'base' is a good starting point.
WHISPER_MODEL_SIZE=base

# The name of the Coqui TTS model to use. A good default is provided.
# Find more models at https://github.com/coqui-ai/TTS/blob/dev/TTS/server/configs/config.json
TTS_MODEL_NAME=tts_models/en/ljspeech/vits

# --- Application Settings ---
# A secret key for signing tokens, etc. Generate a random one for production.
SECRET_KEY=your-super-secret-key-change-me

TWILIO_ACCOUNT_SID=ACxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
TWILIO_AUTH_TOKEN=your_auth_token_from_twilio
TWILIO_PHONE_NUMBER=+15017122661

# --- ADD THESE TWO LINES ---
FASTER_WHISPER_MODEL_SIZE=tiny.en
FASTER_WHISPER_COMPUTE_TYPE=int8